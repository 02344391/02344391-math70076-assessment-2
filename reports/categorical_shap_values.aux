\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{abbrvnat}
\citation{lundberg2019consistent}
\citation{scikit-learn}
\newlabel{eq:shap}{{1}{1}{Introduction}{equation.0.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Plot of the fitted tree. The colored nodes correspond to the paths taken by the algorithm when certain features of an input are known. Here, we consider that no feature is known.}}{2}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:tree0}{{1}{2}{Plot of the fitted tree. The colored nodes correspond to the paths taken by the algorithm when certain features of an input are known. Here, we consider that no feature is known}{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Plot of the fitted tree. The colored nodes correspond to the paths taken by the algorithm when certain features of an input are known. Here, we consider that one feature is known.}}{3}{figure.caption.4}\protected@file@percent }
\newlabel{fig:tree1}{{2}{3}{Plot of the fitted tree. The colored nodes correspond to the paths taken by the algorithm when certain features of an input are known. Here, we consider that one feature is known}{figure.caption.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Plot of the fitted tree. The colored nodes correspond to the paths taken by the algorithm when certain features of an input are known. Here, we consider that two features are known.}}{3}{figure.caption.5}\protected@file@percent }
\newlabel{fig:tree2}{{3}{3}{Plot of the fitted tree. The colored nodes correspond to the paths taken by the algorithm when certain features of an input are known. Here, we consider that two features are known}{figure.caption.5}{}}
\citation{lundberg2019consistent}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Plot of the fitted tree. The colored nodes correspond to the paths taken by the algorithm when certain features of an input are known. Here, we consider that three features are known.}}{4}{figure.caption.6}\protected@file@percent }
\newlabel{fig:tree3}{{4}{4}{Plot of the fitted tree. The colored nodes correspond to the paths taken by the algorithm when certain features of an input are known. Here, we consider that three features are known}{figure.caption.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Average performance of the classification models for datasets containing 1, 3, 5 categorical features.}}{5}{table.caption.8}\protected@file@percent }
\newlabel{table:perfo}{{1}{5}{Average performance of the classification models for datasets containing 1, 3, 5 categorical features}{table.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Boxplots of the absolute difference average of the absolute normalised SHAP values of both methods over the features for each prediction on the training and test set}}{6}{figure.caption.9}\protected@file@percent }
\newlabel{fig:boxplot}{{5}{6}{Boxplots of the absolute difference average of the absolute normalised SHAP values of both methods over the features for each prediction on the training and test set}{figure.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Number of times the rankings differ divided by the total number of predictions for the training and test sets and given the number of categorical data.}}{6}{table.caption.10}\protected@file@percent }
\newlabel{table:rankings}{{2}{6}{Number of times the rankings differ divided by the total number of predictions for the training and test sets and given the number of categorical data}{table.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Prediction scores of the model on the test set (143 observations).}}{7}{table.caption.11}\protected@file@percent }
\newlabel{table:score_titanic}{{3}{7}{Prediction scores of the model on the test set (143 observations)}{table.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Averages of the normalised absolute values of the SHAP values across all observations for each feature in the test set, training set and computed by the two methods.}}{8}{figure.caption.12}\protected@file@percent }
\newlabel{fig:importances_shap}{{6}{8}{Averages of the normalised absolute values of the SHAP values across all observations for each feature in the test set, training set and computed by the two methods}{figure.caption.12}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Number of times the rankings differ divided by the total number of predictions for the training and test sets of the Titanic dataset.}}{8}{table.caption.13}\protected@file@percent }
\newlabel{table:rankings_titanic}{{4}{8}{Number of times the rankings differ divided by the total number of predictions for the training and test sets of the Titanic dataset}{table.caption.13}{}}
\bibdata{bibliography}
\bibcite{lundberg2019consistent}{{1}{2019}{{Lundberg et~al.}}{{Lundberg, Erion, and Lee}}}
\bibcite{scikit-learn}{{2}{2011}{{Pedregosa et~al.}}{{Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, and Duchesnay}}}
\gdef \@abspage@last{10}
